{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Tokenisasi_Padding.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "- change words to numeric format"
      ],
      "metadata": {
        "id": "PTYbfVYT8WjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Import Library"
      ],
      "metadata": {
        "id": "nIIZL7le8bL0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-dqacRYPOftB"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Make objek tokenizer\n",
        "\n",
        "- using ``tokenizer()`` function\n",
        "\n",
        "- num_words : jumlah kata yg akan dikonversi into token/numberic\n",
        "  - if we set num_words: 15 then only 15 word yg paling sering muncul.\n",
        "  - the 15 words akan ditokenisasi dari seluruh kata pada dataset\n",
        "\n",
        "- oov_token : parameter yg berfungsi to change words yg tidak ditokenisasi\n",
        "  - it is better to change unknown words to certain word than melewatkan that words to decrease or prevent lost information "
      ],
      "metadata": {
        "id": "zIGR6llk8ugi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words= 15, oov_token='-')"
      ],
      "metadata": {
        "id": "JBcaJGNnOvLx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. make text dummy for dataset  \n",
        "- for tokenized and \n",
        "- use to trainiing model"
      ],
      "metadata": {
        "id": "mfCNerEhAfn8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    teks = ['Saya suka programming',\n",
        "            'Programming sangat menyenangkan!',\n",
        "            'Machine Learning berbeda dengan pemrograman konvensional ambyar']"
      ],
      "metadata": {
        "id": "AEUbYdEeSHMj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. do tokenization to the dataset"
      ],
      "metadata": {
        "id": "K-s6J7gNCBRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    tokenizer.fit_on_texts(teks)"
      ],
      "metadata": {
        "id": "QOWsrvRMSWrt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- change that tokenized text into sequence"
      ],
      "metadata": {
        "id": "kiD4OntvCe6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    sequences = tokenizer.texts_to_sequences(teks)"
      ],
      "metadata": {
        "id": "fpaPTBF6ScYP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- show word tokenization result  \n",
        "\n",
        "  - using word_index\n",
        "  - it return key (word) : token (numeric) \n",
        "  - tanda baca and huruf kapital tidak diproses. selamat = SELAMAT\n",
        "  - oov (out of vocabulary) diberi token 1"
      ],
      "metadata": {
        "id": "JcSt32wMCq0r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otfQ1PRuSfMK",
        "outputId": "a2770f52-da33-4aaa-cbcb-7d344b97519b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'-': 1, 'programming': 2, 'saya': 3, 'suka': 4, 'sangat': 5, 'menyenangkan': 6, 'machine': 7, 'learning': 8, 'berbeda': 9, 'dengan': 10, 'pemrograman': 11, 'konvensional': 12, 'ambyar': 13}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. change each sentence into suitable token \n",
        "\n",
        "- using ``text_to_sequence()`` \n",
        "- input text parameter"
      ],
      "metadata": {
        "id": "cTtNzpCZGxwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    print(tokenizer.texts_to_sequences(['Saya suka programing']))\n",
        "    print(tokenizer.texts_to_sequences(['Saya suka belajar programing sejak SMP']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tBeh_xGSqK7",
        "outputId": "c7f11e35-7c3f-4de4-ebb1-55505797473a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[3, 4, 1]]\n",
            "[[3, 4, 1, 1, 1, 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. do padding\n",
        "\n",
        "- to make each sequence have similar length\n",
        "- import library ``pad_sequence``\n",
        "- input sequence result hasil tokenisasi as parameter "
      ],
      "metadata": {
        "id": "tXHfltylHaHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "    sequences_samapanjang = pad_sequences(sequences)"
      ],
      "metadata": {
        "id": "vAqKYL8vTAPM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- after do padding, each sequences have same length.\n",
        "- padding can do it by add 0 at beginning of sort sequence 3 4 2 -> 0 0 0 3 4 2"
      ],
      "metadata": {
        "id": "1X-FipbNIq-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences_samapanjang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSrwzZ3DTfMb",
        "outputId": "ae1ba7e7-a6d2-48c5-a082-66aa9568e996"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  3  4  2]\n",
            " [ 0  0  0  0  2  5  6]\n",
            " [ 7  8  9 10 11 12 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- if we want to change 0 is placed at the end sequence.\n",
        "  - we can use padding with post value\n",
        "  - we also can adjust maximun length each sequence by using ``maxlen``.\n",
        "  - if we set 5 as maxlen value then sequence lenght will not more than 5\n",
        "  - if our teks have length more than our parameter then by default value of sequence will be taken last 5 values (ignore words before) -> 9 10 11 12 13\n",
        "    - to adjust to get first 5 values we can use ``truncating='post'``  -> 7 8 9 10 11"
      ],
      "metadata": {
        "id": "R91SNh-8JSxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    sequences_samapanjang = pad_sequences(sequences, \n",
        "                                          padding='post',\n",
        "                                          maxlen=5)"
      ],
      "metadata": {
        "id": "yfrGaCpDTXmM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences_samapanjang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb41a66-3ff7-4d0b-a8f8-3b5c1e7bb35f",
        "id": "lUsPXmAHIp1j"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3  4  2  0  0]\n",
            " [ 2  5  6  0  0]\n",
            " [ 9 10 11 12 13]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    sequences_samapanjang = pad_sequences(sequences, \n",
        "                                          padding='post',\n",
        "                                          maxlen=5,\n",
        "                                          truncating='post')"
      ],
      "metadata": {
        "id": "csceDjDVT6aO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences_samapanjang)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l117ar8VT8aJ",
        "outputId": "1ca4e5c9-01ce-48a4-ac77-aa12788208ba"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 3  4  2  0  0]\n",
            " [ 2  5  6  0  0]\n",
            " [ 7  8  9 10 11]]\n"
          ]
        }
      ]
    }
  ]
}